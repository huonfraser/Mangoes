{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2479f0f0-9d57-4b44-b1c7-a331321f5c71",
   "metadata": {},
   "source": [
    "# Deep Networks with Pytorch-Tabular\n",
    "\n",
    "- branch: master\n",
    "- toc: true \n",
    "- badges: false\n",
    "- comments: false\n",
    "- sticky_rank: 5\n",
    "- author: Huon Fraser\n",
    "- categories: [mangoes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df0eccea-66c4-4308-bfbc-d6421655fcc4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'codetiming'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnotebook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcodetiming\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Timer\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GroupKFold\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscikit_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'codetiming'"
     ]
    }
   ],
   "source": [
    "#collapse-hide\n",
    "import sys\n",
    "sys.path.append('/notebooks/Mangoes/src/')\n",
    "model_path  = '../models/'\n",
    "\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from codetiming import Timer\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from scikit_models import *\n",
    "from skopt.space import Real, Integer\n",
    "from lwr import LocalWeightedRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2331bfaf-b2e6-4c65-91a5-e408219267e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "mangoes=load_mangoes()\n",
    "\n",
    "train_data,test_data = train_test_split(mangoes)\n",
    "train_X, train_y, train_cat = X_y_cat(train_data,min_X=684,max_X=990)\n",
    "test_X, test_y, test_cat = X_y_cat(test_data,min_X=684,max_X=990)\n",
    "nrow,ncol=train_X.shape\n",
    "groups = train_cat['Pop']\n",
    "splitter=GroupKFold()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7986344-5126-4065-b14f-083be6f6228c",
   "metadata": {},
   "source": [
    "Pytorch, the neural-network platform of choice for this project requires users to define their own training and validation loops. While this provides excellent flexability, higher order API's like [pytorch-lighting](https://www.pytorchlightning.ai/) and [fastai](https://www.fast.ai/) cut outmuch of the boilerplate and integrate functionality like callbacks and logging. The [pytorch-tabular](https://pytorch-tabular.readthedocs.io/en/latest/) libary extends the pytorch-lightning API to work better with tabular data.\n",
    "\n",
    "In this notebook we work through building a MLP model, and then training and testing in a manner that is consistent with our earlier sklearn models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26b8a33-f6f1-4eb8-9273-007358ea5401",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from omegaconf import DictConfig\n",
    "from typing import Dict\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.config import DataConfig,OptimizerConfig, TrainerConfig, ExperimentConfig,ModelConfig\n",
    "from pytorch_tabular.models import BaseModel\n",
    "from collections import OrderedDict\n",
    "from pytorch_tabular.models import CategoryEmbeddingModelConfig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc6f2b9-70d3-47d6-b60f-cc29462390a9",
   "metadata": {},
   "source": [
    "## Defining a Model\n",
    "\n",
    "Within pytorch-tabular custom networks can be defined by extending the BaseModel class, which in turn extends the pytorch-lightning LightningModule. All hyperparameters are passed in at iniatialisation by the config paramater and is accessable after super() has been called from self.hparams.\n",
    "\n",
    "Our first step is to write our MLP. We define our network in the \\_build_network function, consisting of two matrices (linear layers) seperated by a ReLU activation layer. The number of inputs and outputs are controlled by hyperparameters inferred from the data while the width of the hidden layer is a user controlled parameter.\n",
    "\n",
    "We also are requred to define the forward class, which controls how data is passed through our network. The input of this function x, consists of a dictionary with continuous and categorical features broken down into x[\"continuous\"] and x[\"categorical\"]. Outputs of forward must be returned in a dictionary with predictions labelled by \"logits\". This is messy (and is unclear in their documentation). Hopefully this is something that will be improved in future iterations of this library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4835cd6-a9a0-4972-b9d5-326fc7c52e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MLPConfig(ModelConfig):\n",
    "    width: int = 10\n",
    "    \n",
    "class MLP(BaseModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: DictConfig,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(config, **kwargs)\n",
    "\n",
    "    def _build_network(self):\n",
    "        layers = OrderedDict({'layer_1':nn.Linear(self.hparams[\"continuous_dim\"], self.hparams[\"width\"]),\n",
    "                             # 'act_1':nn.ReLU(),\n",
    "                              'layer_2':nn.Linear(self.hparams[\"width\"],self.hparams[\"output_dim\"])\n",
    "        })\n",
    "        self.model = nn.Sequential(layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x[\"continuous\"]\n",
    "        y_hat=  self.model.forward(x)\n",
    "        return  {'logits':y_hat}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a640d8-595e-415c-be0b-f97f34afae11",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configurations\n",
    "\n",
    "Data is expected to be a single pd.DataFram including both X and y. This is a departure from the sklearn approach, and in the future we'll work on a fix for this.\n",
    "For the time being, we merge our X and y and define the names of our categorical and numerical columns. We pass this metadata into a DataConfig object, which handles loading and transforming data for us.\n",
    "\n",
    "Similarly we also define a TrainerConfig class and an OptimizerConfig class, which defines all the hyperparmeters controlling training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6134f8a5-a2f8-4505-91a1-7762db97f559",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_col_names = train_X.columns.tolist()\n",
    "cat_col_names = []\n",
    "\n",
    "train_Xy = deepcopy(train_X)\n",
    "test_Xy = deepcopy(test_y)\n",
    "train_Xy['target']=train_y\n",
    "test_Xy['target']=test_y\n",
    "\n",
    "data_config = DataConfig(\n",
    "    target=['target'], #target should always be a list. Multi-targets are only supported for regression. Multi-Task Classification is not implemented\n",
    "    continuous_cols=num_col_names,\n",
    "    categorical_cols=cat_col_names,\n",
    ")\n",
    "trainer_config = TrainerConfig(\n",
    "    auto_lr_find=False, # Runs the LRFinder to automatically derive a learning rate\n",
    "    batch_size=32,\n",
    "    max_epochs=100,\n",
    "    gpus=-1, #index of the GPU to use. -1 means all available GPUs, None, means CPU\n",
    ")\n",
    "\n",
    "model_config = TestNetConfig(task=\"regression\",\n",
    "                            learning_rate = 1e-3)\n",
    "\n",
    "optimizer_config = OptimizerConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d3d7ad-829d-4b6e-b67b-58961e8aa6ee",
   "metadata": {},
   "source": [
    "All the pieces are assembled in the TabularModel class. As well as our Config classes, we also define the model_callable to be a reference to our MLP class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e74552-6250-4ece-9e51-4899129d3104",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tabular_model = TabularModel(\n",
    "    data_config=data_config,\n",
    "    model_config=model_config,\n",
    "    optimizer_config=optimizer_config,\n",
    "    trainer_config=trainer_config,\n",
    "    model_callable = MLP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1d764d-eb24-4124-a628-2282c0c9d497",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_model.fit(train=train_Xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1ca539-964c-4710-8930-30188407035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_model.evaluate(test_Xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92238bd-c6b0-4215-b467-06a0c7bf65d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo check outputs of tabular_model.evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe10bff6-12e8-45e7-aced-7c8d5e46c6fc",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "\n",
    "So far we have implemented a minimum-viable model and evaluation. We now wrap this into a cross-validation framework.\n",
    "\n",
    "First we need to consider a key design choice. For our sklearn models, cross-validation led to building multiple versions of a model with slightly different parameters. For neural networks, with optimising being a highly stochastic gradient descent path, there is no gurantee that cross-validation folds are similar, or that folds resemble the final model trained on all the data. After running cross-validation, we let the user define how to build the final model; None, for building no final model to save time, \"All\", to train a model on the whole training set, or \"Ensemble\", to build an ensemble on the cross-validation folds. \n",
    "\n",
    "We define our ensemble implementation below. At the moment we just pass in each model. IN the future we may instead pass in a location of a savefile for each model, or a single model and a list of locations to get weights from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025b13a1-160c-4627-b39b-bbd1661c048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EnsembleConfig(ModelConfig):\n",
    "    models: list  = []\n",
    "\n",
    "class Ensemble(EnsembleModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: DictConfig,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(config, **kwargs)\n",
    "\n",
    "    def _build_network(self):\n",
    "        self.models = self.hparams[\"continuous_dim\"]\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x[\"continuous\"]\n",
    "        y_hats=torch.zeros(x.shape[1],len(self.models))\n",
    "        for i,model in enumerate(models):\n",
    "            y_hats[:,i]= self.model.forward(x)['logits']\n",
    "        y_hat = torch.mean(y_hats)\n",
    "        return  {'logits':y_hat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b341412f-bc87-46e9-a9f9-d7e602b8605f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GroupKFold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deepcopy\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcross_validate\u001b[39m(model,X,y,splitter\u001b[38;5;241m=\u001b[39m\u001b[43mGroupKFold\u001b[49m(),groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,plot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,save_loc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m#combine X and y\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     Xy \u001b[38;5;241m=\u001b[39m deepcopy(X)\n\u001b[1;32m      6\u001b[0m     Xy[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39my\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GroupKFold' is not defined"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import GroupKFold, KFold\n",
    "\n",
    "def cross_validate(model,X,y,splitter=GroupKFold(),groups=None,plot=False,save_loc=None,final_model=\"Ensemble\"): #Ensemble, \"All\", None\n",
    "    #combine X and y\n",
    "    Xy = deepcopy(X)\n",
    "    Xy[\"target\"]=y\n",
    "    \n",
    "    preds = None\n",
    "    ys = None\n",
    "    models = []\n",
    "    for fold, (inds1,inds2) in enumerate(splitter.split(X,y,groups)):\n",
    "        \n",
    "        model.fit(X.iloc[inds1,:],y.iloc[inds1,:])\n",
    "        pred = model.predict(X.iloc[inds2,:])\n",
    "\n",
    "        if preds is None:\n",
    "            preds = pred\n",
    "            ys = y.iloc[inds2,:]\n",
    "        else:\n",
    "            preds = np.concatenate((preds,pred),axis=0)\n",
    "            ys = np.concatenate((ys,y.iloc[inds2,:]),axis=0)\n",
    "            \n",
    "        if final_model == \"Ensemble\":\n",
    "            models.append(deepcopy(model))\n",
    "            \n",
    "\n",
    "    r2 = r2_score(ys,preds)\n",
    "    mse = mean_squared_error(ys,preds)\n",
    "\n",
    "    if plot:\n",
    "        ys = ys.flatten()\n",
    "        preds = preds.flatten()\n",
    "\n",
    "        m, b = np.polyfit(ys, preds, 1)\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        ls = np.linspace(min(ys),max(ys))\n",
    "        ax.plot(ls,ls*m+b,color = \"black\", label = r\"$\\hat{y}$ = \"+f\"{m:.4f}y + {b:.4f}\")\n",
    "        ax.scatter(x=ys,y=preds,label = r\"$R^2$\" + f\"={r2:.4f}\")\n",
    "\n",
    "        ax.set_xlabel('True Values')\n",
    "        ax.set_ylabel('Predicted Values')\n",
    "        ax.legend(bbox_to_anchor=(0.5,1))\n",
    "        if not save_loc is None:\n",
    "            fig.savefig(save_loc)\n",
    "            \n",
    "    if final_model == \"Ensemble\":\n",
    "         #create new tabular model, passing in same configs but with an ensemble\n",
    "    elif final_model == \"All\": #train final model on all data\n",
    "        model = model.fit(x,y)\n",
    "    else: #ignore training the final model, for computation saving purposes \n",
    "        model = None\n",
    "    \n",
    "    return model, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23034cb4-932d-4e62-8b25-101a9c15bfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,train_X,train_y,test_X,test_y,plot=False,save_loc=None,log=True):\n",
    "    test_y=test_y.values.flatten()\n",
    "    model.fit(train_X,train_y)\n",
    "    preds = model.predict(test_X)\n",
    "\n",
    "    r2 = r2_score(test_y,preds)\n",
    "    mse = mean_squared_error(test_y,preds)\n",
    "\n",
    "    if log:\n",
    "        print(f\"Test set MSE: {mse:.4f}\")\n",
    "\n",
    "    if plot:\n",
    "        preds=preds.flatten()\n",
    "\n",
    "        m, b = np.polyfit(test_y, preds, 1)\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        ls = np.linspace(min(test_y),max(test_y))\n",
    "        ax.plot(ls,ls*m+b,color = \"black\", label = r\"$\\hat{y}$ = \"+f\"{m:.4f}y + {b:.4f}\")\n",
    "        ax.scatter(x=test_y,y=preds,label = r\"$R^2$\" + f\"={r2:.4f}\")\n",
    "\n",
    "        ax.set_xlabel('True Values')\n",
    "        ax.set_ylabel('Predicted Values')\n",
    "        ax.legend(bbox_to_anchor=(0.5,1))\n",
    "        if not save_loc is None:\n",
    "            fig.savefig(save_loc)\n",
    "    return model, mse "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
